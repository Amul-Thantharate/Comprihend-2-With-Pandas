{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            Scheduling an Analysis Job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I highly recommend that you run at least one Comprehend job from the point and click interface, especially, if you are new to AWS. This way you can create a data access role (aka data_access_role_arn), and then you can simply copy the role name from the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file uploaded to comprihend-tweet-bucket bucket with name input-data/amazon_tweets_1.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError \n",
    "from dotenv import load_dotenv\n",
    "import json \n",
    "import pandas as pd \n",
    "import tarfile\n",
    "load_dotenv()\n",
    "\n",
    "s3 = boto3.client('s3',)\n",
    "\n",
    "bucket_name = \"comprihend-tweet-bucket\"\n",
    "local_file_name = \"D:\\\\Scripts\\\\Comprihend\\\\Comprihend-2-With-Pandas\\\\Comprehend\\\\amazon_tweets.csv\"\n",
    "aws_file_name = \"input-data/amazon_tweets_1.csv\"\n",
    "output = s3.upload_file(local_file_name, bucket_name, aws_file_name)\n",
    "print(f\"file uploaded to {bucket_name} bucket with name {aws_file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Entity Detection Job: 2acf35038f261168fc332295f18bd0ec\n"
     ]
    }
   ],
   "source": [
    "input_s3_url = \"s3://comprihend-tweet-bucket/input-data\"\n",
    "input_doc_format = \"ONE_DOC_PER_LINE\"\n",
    "output_s3_url = \"s3://comprihend-tweet-bucket/output-data\"\n",
    "data_access_role_arn = \"arn:aws:iam::XXXXXXXXXXXXXXX:role/service-role/AmazonComprehendServiceRole-Com-S3\" # replace with your IAM role arn \n",
    "number_of_topics = 10\n",
    "\n",
    "input_data_config = {\n",
    "    'S3Uri': input_s3_url,\n",
    "    'InputFormat': input_doc_format,\n",
    "}\n",
    "output_data_config = {\n",
    "    'S3Uri': output_s3_url,\n",
    "}\n",
    "comprehend  = boto3.client('comprehend')\n",
    "start_job_entity = comprehend.start_entities_detection_job(\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_access_role_arn,\n",
    "    LanguageCode='en'\n",
    ")\n",
    "\n",
    "job_id = start_job_entity['JobId']\n",
    "print(f'Started Entity Detection Job: {job_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "describe_result = comprehend.describe_entities_detection_job(JobId=job_id)\n",
    "job_status = describe_result['EntitiesDetectionJobProperties']['JobStatus']\n",
    "print(f'Job Status: {job_status}')\n",
    "if job_status == 'FAILED':\n",
    "    print(f'Reason: {describe_result[\"EntitiesDetectionJobProperties\"][\"Message\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://comprihend-tweet-bucket/output-data/893415859041-NER-2acf35038f261168fc332295f18bd0ec/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# job_id = \"db6a3a8cf2541530bb96827b8d1a7edc\" \n",
    "enitity_result = comprehend.describe_entities_detection_job(JobId=job_id)['EntitiesDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print(enitity_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-data/\n",
      "input-data/amazon_tweets_1.csv\n",
      "output-data/.write_access_check_file.temp\n",
      "output-data/893415859041-NER-2acf35038f261168fc332295f18bd0ec/output/output.tar.gz\n",
      "output-data/893415859041-NER-654f93bda25adaa33dfe3bae93cf980c/output/output.tar.gz\n",
      "results/\n",
      "results/.write_access_check_file.temp\n"
     ]
    }
   ],
   "source": [
    "# list all files and folders in the bucket \n",
    "def list_all_files():\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.all():\n",
    "        print(obj.key)\n",
    "list_all_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                        Downloading the Results From S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file: output-data/893415859041-NER-2acf35038f261168fc332295f18bd0ec/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3') \n",
    "bucket_name = \"comprihend-tweet-bucket\"\n",
    "s3_name = 's3://' + bucket_name + '/' \n",
    "result_aws_file_name = enitity_result.replace(s3_name, '')\n",
    "local_file_name = 'Comprehend\\\\outputs\\\\entities.tar.gz'\n",
    "\n",
    "s3.download_file(bucket_name, result_aws_file_name, local_file_name)\n",
    "\n",
    "print('Downloaded file: ' + result_aws_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                ## Extracting the Entities from the Results ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Comprehend\\outputs\\entities.tar.gz to Comprehend\\outputs\\extracted\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "def extract_targz(targz_file, output_path = ''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "output_path = 'Comprehend\\\\outputs\\\\extracted'\n",
    "extract_targz(local_file_name, output_path)\n",
    "print(f\"Extracted {local_file_name} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# input_file_name = 'Comprehend\\\\outputs\\\\extracted\\\\output'\n",
    "input_file_name = 'Comprehend\\outputs\\extracted\\output'\n",
    "entites = [\n",
    "    json.loads(line) for line in open(input_file_name, 'r')\n",
    "]\n",
    "len(entites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function parses entities data into a dataframe\n",
    "def parse_entities(data):\n",
    "    df = pd.DataFrame() # declare an empty dataframe\n",
    "    nested_json = 'Entities' # nested sub-dictiptionary to extract data from\n",
    "    # populate the dataframe\n",
    "    for line in data:\n",
    "        dt_temp = pd.DataFrame(line[nested_json])  # extract data from sub-dictionary\n",
    "        other_fields = list(line.keys())\n",
    "        other_fields.remove(nested_json) # remove nested fields        \n",
    "        for field in other_fields:  # add common fields\n",
    "            dt_temp[field] = line[field]\n",
    "        \n",
    "        df = pd.DataFrame(pd.concat([df, dt_temp], sort=False))\n",
    "    return(df)\n",
    "entities_df = parse_entities(entites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win 5 High Value Amazon Vouchers in @gadgetstouse giveaway, you need to watch this video at https://t.co/7nE81BVVjq - make sure to leave a comment and like the video.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>File</th>\n",
       "      <th>Line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.951132</td>\n",
       "      <td>5 High Value</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>amazon_tweets_1.csv</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.957802</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>amazon_tweets_1.csv</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.470461</td>\n",
       "      <td>gadgetstouse</td>\n",
       "      <td>TITLE</td>\n",
       "      <td>amazon_tweets_1.csv</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.993396</td>\n",
       "      <td>https://t.co/7nE81BVVjq</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>amazon_tweets_1.csv</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BeginOffset  EndOffset     Score                     Text          Type  \\\n",
       "0          5.0       17.0  0.951132             5 High Value      QUANTITY   \n",
       "1         18.0       24.0  0.957802                   Amazon  ORGANIZATION   \n",
       "2         38.0       50.0  0.470461             gadgetstouse         TITLE   \n",
       "3         93.0      116.0  0.993396  https://t.co/7nE81BVVjq         OTHER   \n",
       "\n",
       "                  File  Line  \n",
       "0  amazon_tweets_1.csv   999  \n",
       "1  amazon_tweets_1.csv   999  \n",
       "2  amazon_tweets_1.csv   999  \n",
       "3  amazon_tweets_1.csv   999  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:\\\\Scripts\\\\Comprihend\\\\Comprihend-2-With-Pandas\\\\Comprehend\\\\amazon_tweets.csv', names=['amazon_tweets'], header=None, dtype= 'str')\n",
    "record_no = 999\n",
    "# Tweet text\n",
    "print(df.loc[record_no].item())\n",
    "# Resutls\n",
    "entities_df.query('Line == @record_no')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
