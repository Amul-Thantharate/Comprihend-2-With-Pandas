{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                          ####  Using Amazon Comprehend Through the boto3 API ####\n",
    "\n",
    "This notebook shows how to use boto3 Amazon API to use Amazon Comprehend for real time analysis as well as scheduling analysis jobs.\n",
    "\n",
    "For boto3 to work you need to create an IAM User, receive aws_access_key_id and aws_secret_access_key and configure your credentials using AWS Command Line Interface (AWS CLI).\n",
    "Cost. If you are using free AWS tier, you can analyze 50K units a month free. In my example, every tweet is a unit. In the scheduled job I am analyzing 10K tweets at once, so the free tier runs out pretty fast, and then it's $1 per 10K. Be sure to check pricing before you proceed. https://aws.amazon.com/comprehend/pricing/\n",
    "Reference. Boto3 S3: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/comprehend.html Boto3 Comprehend: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import boto3\n",
    "import pandas as pd \n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError\n",
    "import tarfile \n",
    "import json \n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified then file_name is used\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Upload the file\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "bucket_name = \"comprihend-tweet-bucket\"\n",
    "local_file_name = \"Comprehend\\\\amazon_tweets_1.csv\"\n",
    "s3_file_name = \"amazon_tweets.csv\"\n",
    "upload_file(local_file_name, bucket_name, s3_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                        ### Downloading the data from the S3 bucket in the form of .tar.gz file ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_tweets.csv\n",
      "File downloaded from https://comprihend-tweet-bucket.s3.amazonaws.com/amazon_tweets_1.csv to Comprehend/outputs/entities.csv\n"
     ]
    }
   ],
   "source": [
    "entities_results_S3Url = \"https://comprihend-tweet-bucket.s3.amazonaws.com/amazon_tweets_1.csv\"\n",
    "local_results_filename = 'Comprehend/outputs/entities.csv'\n",
    "\n",
    "s3_name = 's3://' + bucket_name + '/'\n",
    "results_aws_filename = entities_results_S3Url.replace(s3_name, '')\n",
    "def download_all_files():\n",
    "    s3 = boto3.resource('s3')\n",
    "    my_bucket = s3.Bucket(bucket_name)\n",
    "    for file in my_bucket.objects.all():\n",
    "        print(file.key)\n",
    "        s3.Bucket(bucket_name).download_file(file.key, local_results_filename)\n",
    "            \n",
    "download_all_files()\n",
    "print(f\"File downloaded from {entities_results_S3Url} to {local_results_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            ### Extracting the result ###    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file extracted to Comprehend/outputs/extracted\n"
     ]
    }
   ],
   "source": [
    "def extract_targz(targz_file, output_path = ''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "output_path = 'Comprehend/outputs/extracted'\n",
    "extract_targz(local_results_filename, output_path)\n",
    "print(f\"file extracted to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                ### Live Single Record Processing ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 0.28518974781036377,\n",
       " 'Negative': 0.0012132528936490417,\n",
       " 'Neutral': 0.7135248184204102,\n",
       " 'Mixed': 7.219231338240206e-05}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_file_name = 'Comprehend/amazon_tweets.csv'\n",
    "df = pd.read_csv(local_file_name, header = None, names = ['amazon_tweets'], dtype = 'str')\n",
    "df.loc[0].item()\n",
    "comprehend = boto3.client(service_name='comprehend')\n",
    "sentiment_output = comprehend.detect_sentiment(Text=df.loc[0].item(), LanguageCode='en')\n",
    "# print(sentiment_output)\n",
    "sentiment_output['SentimentScore']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    ### Live Multiple Record Processing ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet25 = list(df.amazon_tweets[0:5])\n",
    "for i in range(len(tweet25)):\n",
    "    sentiment_output = comprehend.detect_sentiment(Text=tweet25[i], LanguageCode='en')\n",
    "    print(sentiment_output)\n",
    "    print(sentiment_output['SentimentScore']) \n",
    "    # write a output in a file \n",
    "    with open('sentiment_output.txt', 'a') as f:\n",
    "        f.write(str(sentiment_output['SentimentScore']))\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "tweets25 = list(df.amazon_tweets[0:6])\n",
    "sentiment_batch = comprehend.batch_detect_sentiment(TextList=tweets25,\n",
    "                                                    LanguageCode='en')\n",
    "tweets25[4]\n",
    "sentiment_batch['ResultList'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997289</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>USA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992791</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>FDA</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.856835</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>GMP</td>\n",
       "      <td>33</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.874060</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.945508</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>every time</td>\n",
       "      <td>122</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994793</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/i6ZwFpeo4p</td>\n",
       "      <td>134</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.565103</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/XRU2MvKLVy</td>\n",
       "      <td>158</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score          Type                     Text  BeginOffset  EndOffset\n",
       "0  0.997289      LOCATION                      USA           12         15\n",
       "1  0.992791  ORGANIZATION                      FDA           17         20\n",
       "2  0.856835  ORGANIZATION                      GMP           33         36\n",
       "3  0.874060  ORGANIZATION                   Kosher           41         47\n",
       "4  0.945508      QUANTITY               every time          122        132\n",
       "5  0.994793         OTHER  https://t.co/i6ZwFpeo4p          134        157\n",
       "6  0.565103         OTHER  https://t.co/XRU2MvKLVy          158        181"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = comprehend.batch_detect_entities(TextList=tweets25, LanguageCode='en')\n",
    "tweets25_entities = entities['ResultList']\n",
    "pd.DataFrame(entities['ResultList'][0]['Entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997289</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>USA</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.992791</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>FDA</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.856835</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>GMP</td>\n",
       "      <td>33</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.874060</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Kosher</td>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.945508</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>every time</td>\n",
       "      <td>122</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.994793</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/i6ZwFpeo4p</td>\n",
       "      <td>134</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.565103</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>https://t.co/XRU2MvKLVy</td>\n",
       "      <td>158</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.997085</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>$200</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.949746</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.962676</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>@SenSanders</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.896190</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>amazon</td>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.920686</td>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>first</td>\n",
       "      <td>128</td>\n",
       "      <td>133</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992122</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Bezos</td>\n",
       "      <td>164</td>\n",
       "      <td>169</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999554</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>Ray Gricar</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.988366</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>Penn State</td>\n",
       "      <td>56</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Score          Type                     Text  BeginOffset  EndOffset  \\\n",
       "0  0.997289      LOCATION                      USA           12         15   \n",
       "1  0.992791  ORGANIZATION                      FDA           17         20   \n",
       "2  0.856835  ORGANIZATION                      GMP           33         36   \n",
       "3  0.874060  ORGANIZATION                   Kosher           41         47   \n",
       "4  0.945508      QUANTITY               every time          122        132   \n",
       "5  0.994793         OTHER  https://t.co/i6ZwFpeo4p          134        157   \n",
       "6  0.565103         OTHER  https://t.co/XRU2MvKLVy          158        181   \n",
       "0  0.997085      QUANTITY                     $200            0          4   \n",
       "1  0.949746  ORGANIZATION                   Amazon            5         11   \n",
       "0  0.962676        PERSON              @SenSanders            0         11   \n",
       "1  0.896190  ORGANIZATION                   amazon           51         57   \n",
       "2  0.920686      QUANTITY                    first          128        133   \n",
       "3  0.992122        PERSON                    Bezos          164        169   \n",
       "0  0.999554        PERSON               Ray Gricar           31         41   \n",
       "1  0.988366  ORGANIZATION               Penn State           56         66   \n",
       "\n",
       "   Index  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "0      1  \n",
       "1      1  \n",
       "0      2  \n",
       "1      2  \n",
       "2      2  \n",
       "3      2  \n",
       "0      3  \n",
       "1      3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to parse the dictionary\n",
    "def parse_entities_batch(data):\n",
    "    df = pd.DataFrame() # declare an empty dataframe\n",
    "    nested_json = 'Entities' # nested sub-dictiptionary to extract data from\n",
    "    # populate the dataframe\n",
    "    for line in data['ResultList']:\n",
    "        dt_temp = pd.DataFrame(line[nested_json])  # extract data from sub-dictionary\n",
    "        other_fields = list(line.keys())\n",
    "        other_fields.remove(nested_json) # remove nested fields        \n",
    "        for field in other_fields:  # add common fields\n",
    "            dt_temp[field] = line[field]\n",
    "            df = pd.DataFrame(pd.concat([df, dt_temp], sort=False))\n",
    "    return(df)\n",
    "entities_batch_df = parse_entities_batch(entities)\n",
    "entities_batch_df.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Index': 4,\n",
       " 'Sentiment': 'NEUTRAL',\n",
       " 'SentimentScore': {'Positive': 0.3611469268798828,\n",
       "  'Negative': 0.00013270163617562503,\n",
       "  'Neutral': 0.6384015679359436,\n",
       "  'Mixed': 0.0003188896516803652}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_batch = comprehend.batch_detect_sentiment(TextList=tweets25,\n",
    "                                                    LanguageCode='en')\n",
    "tweets25[4]\n",
    "sentiment_batch['ResultList'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Mixed</th>\n",
       "      <th>Index</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.285190</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.713525</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.005474</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.994367</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.941665</td>\n",
       "      <td>0.056931</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>2</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.011450</td>\n",
       "      <td>0.526684</td>\n",
       "      <td>0.461734</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.361147</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.638402</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>4</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Positive  Negative   Neutral     Mixed  Index Sentiment\n",
       "0  0.285190  0.001213  0.713525  0.000072      0   NEUTRAL\n",
       "1  0.005474  0.000151  0.994367  0.000007      1   NEUTRAL\n",
       "2  0.001153  0.941665  0.056931  0.000251      2  NEGATIVE\n",
       "3  0.011450  0.526684  0.461734  0.000131      3  NEGATIVE\n",
       "4  0.361147  0.000133  0.638402  0.000319      4   NEUTRAL"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_sentiment_batch(data):\n",
    "    df = pd.DataFrame() \n",
    "    for line in data['ResultList']:\n",
    "        try:\n",
    "            dt_temp = pd.DataFrame(line['SentimentScore'], index = [0])  # extract data from sub-dictionary\n",
    "            for field in list(line.keys())[:-1]:  # add common fields\n",
    "                dt_temp[field] = line[field]\n",
    "        \n",
    "            df = pd.DataFrame(pd.concat([df, dt_temp], sort=False, ignore_index=True))\n",
    "                  \n",
    "        except:\n",
    "            for field in list(line.keys())[:-1]:  # add common fields\n",
    "                dt_temp[field] = line[field]\n",
    "        \n",
    "            df = pd.DataFrame(dt_temp, ignore_index = True)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "sentiment_batch_df = parse_sentiment_batch(sentiment_batch)\n",
    "\n",
    "sentiment_batch_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            ### Key Phrases ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>BeginOffset</th>\n",
       "      <th>EndOffset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.985102</td>\n",
       "      <td>the USA</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.753778</td>\n",
       "      <td>FDA registered, GMP and Kosher certified facility</td>\n",
       "      <td>17</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974916</td>\n",
       "      <td>the highest quality, potency and consistency</td>\n",
       "      <td>77</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.993210</td>\n",
       "      <td>every time</td>\n",
       "      <td>122</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.802611</td>\n",
       "      <td>https://t.co/i6ZwFpeo4p https://t.co/XRU2MvKLV...</td>\n",
       "      <td>134</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.773343</td>\n",
       "      <td>Gluten Free Giveaways</td>\n",
       "      <td>221</td>\n",
       "      <td>242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.527550</td>\n",
       "      <td>SenSanders</td>\n",
       "      <td>243</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.997508</td>\n",
       "      <td>people</td>\n",
       "      <td>272</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.983171</td>\n",
       "      <td>amazon</td>\n",
       "      <td>293</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.988972</td>\n",
       "      <td>the richest</td>\n",
       "      <td>313</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.996863</td>\n",
       "      <td>a simple plan</td>\n",
       "      <td>338</td>\n",
       "      <td>351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.999625</td>\n",
       "      <td>people</td>\n",
       "      <td>381</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.996053</td>\n",
       "      <td>Bezos</td>\n",
       "      <td>406</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.991266</td>\n",
       "      <td>his products</td>\n",
       "      <td>477</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.997375</td>\n",
       "      <td>Ray Gricar</td>\n",
       "      <td>530</td>\n",
       "      <td>540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.995214</td>\n",
       "      <td>Penn State</td>\n",
       "      <td>555</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.998194</td>\n",
       "      <td>Marjorie Diehl</td>\n",
       "      <td>576</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.651071</td>\n",
       "      <td>ups</td>\n",
       "      <td>597</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.719999</td>\n",
       "      <td>https://t.co/eIrjAFD7hz FBI</td>\n",
       "      <td>602</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.579442</td>\n",
       "      <td>Reports</td>\n",
       "      <td>641</td>\n",
       "      <td>648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.810117</td>\n",
       "      <td>SASB.@MainZane1 Thanks</td>\n",
       "      <td>660</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.666890</td>\n",
       "      <td>Grand Summoners #goldenkamuy #instantwin #swee...</td>\n",
       "      <td>696</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.497427</td>\n",
       "      <td>video</td>\n",
       "      <td>763</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.899491</td>\n",
       "      <td>$100 Amazon gift card!</td>\n",
       "      <td>789</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.402276</td>\n",
       "      <td>Retweet</td>\n",
       "      <td>813</td>\n",
       "      <td>820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.389755</td>\n",
       "      <td>everyday</td>\n",
       "      <td>821</td>\n",
       "      <td>829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.568220</td>\n",
       "      <td>chance</td>\n",
       "      <td>842</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.675640</td>\n",
       "      <td>GS Global</td>\n",
       "      <td>864</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.660216</td>\n",
       "      <td>FREE ★5 Golden Kamuy Unit!</td>\n",
       "      <td>884</td>\n",
       "      <td>910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.581613</td>\n",
       "      <td>https://t.co/s4Pk2fl3dI@MyLesbianAss Game of</td>\n",
       "      <td>913</td>\n",
       "      <td>957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.617494</td>\n",
       "      <td>Expanse  ***FINAL WARS TRILOGY*** One man</td>\n",
       "      <td>976</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.447169</td>\n",
       "      <td>collapse of</td>\n",
       "      <td>1037</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.435709</td>\n",
       "      <td>23rd</td>\n",
       "      <td>1069</td>\n",
       "      <td>1073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.533282</td>\n",
       "      <td>century.</td>\n",
       "      <td>1074</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.665146</td>\n",
       "      <td>Extraordinary characters</td>\n",
       "      <td>1085</td>\n",
       "      <td>1109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.628635</td>\n",
       "      <td>taut, rousing futuristic tale.”</td>\n",
       "      <td>1118</td>\n",
       "      <td>1149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.659392</td>\n",
       "      <td>Kirkus Reviews  https://t.co/x4Zwe5BTTG</td>\n",
       "      <td>1152</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.573585</td>\n",
       "      <td>3 FREE</td>\n",
       "      <td>1196</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score                                               Text  BeginOffset  \\\n",
       "0   0.985102                                            the USA            8   \n",
       "1   0.753778  FDA registered, GMP and Kosher certified facility           17   \n",
       "2   0.974916       the highest quality, potency and consistency           77   \n",
       "3   0.993210                                         every time          122   \n",
       "4   0.802611  https://t.co/i6ZwFpeo4p https://t.co/XRU2MvKLV...          134   \n",
       "5   0.773343                              Gluten Free Giveaways          221   \n",
       "6   0.527550                                         SenSanders          243   \n",
       "7   0.997508                                             people          272   \n",
       "8   0.983171                                             amazon          293   \n",
       "9   0.988972                                        the richest          313   \n",
       "10  0.996863                                      a simple plan          338   \n",
       "11  0.999625                                             people          381   \n",
       "12  0.996053                                              Bezos          406   \n",
       "13  0.991266                                       his products          477   \n",
       "14  0.997375                                         Ray Gricar          530   \n",
       "15  0.995214                                         Penn State          555   \n",
       "16  0.998194                                     Marjorie Diehl          576   \n",
       "17  0.651071                                                ups          597   \n",
       "18  0.719999                        https://t.co/eIrjAFD7hz FBI          602   \n",
       "19  0.579442                                            Reports          641   \n",
       "20  0.810117                             SASB.@MainZane1 Thanks          660   \n",
       "21  0.666890  Grand Summoners #goldenkamuy #instantwin #swee...          696   \n",
       "22  0.497427                                              video          763   \n",
       "23  0.899491                             $100 Amazon gift card!          789   \n",
       "24  0.402276                                            Retweet          813   \n",
       "25  0.389755                                           everyday          821   \n",
       "26  0.568220                                             chance          842   \n",
       "27  0.675640                                          GS Global          864   \n",
       "28  0.660216                         FREE ★5 Golden Kamuy Unit!          884   \n",
       "29  0.581613       https://t.co/s4Pk2fl3dI@MyLesbianAss Game of          913   \n",
       "30  0.617494          Expanse  ***FINAL WARS TRILOGY*** One man          976   \n",
       "31  0.447169                                        collapse of         1037   \n",
       "32  0.435709                                               23rd         1069   \n",
       "33  0.533282                                           century.         1074   \n",
       "34  0.665146                           Extraordinary characters         1085   \n",
       "35  0.628635                    taut, rousing futuristic tale.”         1118   \n",
       "36  0.659392            Kirkus Reviews  https://t.co/x4Zwe5BTTG         1152   \n",
       "37  0.573585                                             3 FREE         1196   \n",
       "\n",
       "    EndOffset  \n",
       "0          15  \n",
       "1          66  \n",
       "2         121  \n",
       "3         132  \n",
       "4         211  \n",
       "5         242  \n",
       "6         253  \n",
       "7         278  \n",
       "8         299  \n",
       "9         324  \n",
       "10        351  \n",
       "11        387  \n",
       "12        411  \n",
       "13        489  \n",
       "14        540  \n",
       "15        565  \n",
       "16        590  \n",
       "17        600  \n",
       "18        629  \n",
       "19        648  \n",
       "20        682  \n",
       "21        749  \n",
       "22        768  \n",
       "23        811  \n",
       "24        820  \n",
       "25        829  \n",
       "26        848  \n",
       "27        873  \n",
       "28        910  \n",
       "29        957  \n",
       "30       1017  \n",
       "31       1048  \n",
       "32       1073  \n",
       "33       1082  \n",
       "34       1109  \n",
       "35       1149  \n",
       "36       1191  \n",
       "37       1202  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_dump = [''.join(tweets25)]\n",
    "key_prase_batch_output = comprehend.batch_detect_key_phrases(TextList=tweet_dump, LanguageCode='en')\n",
    "pd.DataFrame(key_prase_batch_output['ResultList'][0]['KeyPhrases']) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
