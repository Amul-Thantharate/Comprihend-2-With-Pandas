{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            ***  Asynchronous Processing - Scheduling an Analysis Job  ***\n",
    "I highly recommend that you run at least one Comprehend job from the point and click interface, especially, if you are new to AWS. This way you can create a data access role (aka data_access_role_arn), and then you can simply copy the role name from the job description.\n",
    "\n",
    "You will need to create your S3 bucket through the web interface or through BOTO3 API.\n",
    "\n",
    "Note that I use different folders s3://comprehend-api/input-data and s3://comprehend-api/results for input data and results output. This way, your results are not going to get confused for inputs if you were to analyze all files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket name is Allready exist\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import json \n",
    "import pandas as pd \n",
    "from botocore.exceptions import ClientError \n",
    "load_dotenv()\n",
    "import logging\n",
    "import tarfile\n",
    "# print(load_dotenv())\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    \"\"\"Create an S3 bucket in a specified region\n",
    "    If a region is not specified, the bucket is created in the S3 default\n",
    "    region (us-east-1).\n",
    "    :param bucket_name: Bucket to create\n",
    "    :param region: String region to create bucket in, e.g., 'us-west-2'\n",
    "    :return: True if bucket created, else False\n",
    "    \"\"\" \n",
    "    if bucket_name != None:\n",
    "        region = 'us-east-1'\n",
    "        s3_client = boto3.client('s3', region_name=region)\n",
    "        s3_client.create_bucket(Bucket=bucket_name)\n",
    "        return True\n",
    "    elif bucket_name == None:\n",
    "        print(\"Bucket name is empty\")\n",
    "        \n",
    "        return False\n",
    "bucket_name = \"comphrend-wallmart-bucket\"\n",
    "if bucket_name == \"comphrend-wallmart-bucket\":\n",
    "    print(\"Bucket name is Allready exist\")\n",
    "    pass\n",
    "else:\n",
    "    create_bucket(bucket_name)\n",
    "    print(\"Bucket created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    *** Uploding the data to S3 *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Uploaded\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "local_file_name = \"../Comprehend/wallmarts_tweets_1k.csv\"\n",
    "bucket_name = \"comphrend-wallmart-bucket\"\n",
    "aws_file_name = \"input-data/wallmarts_tweets_1k.csv\"\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "def upload_file(local_file_name, bucket_name, aws_file_name):\n",
    "    try:\n",
    "        s3 = s3_client.upload_file(local_file_name, bucket_name, aws_file_name)\n",
    "        print(\"File Uploaded\")\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "    return True\n",
    "print(upload_file(local_file_name, bucket_name, aws_file_name)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                *** Configure Sentiment Detection Job *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: 7789fcaa43d6874376332374a69fe845\n"
     ]
    }
   ],
   "source": [
    "comprehend = boto3.client('comprehend')\n",
    "input_s3_url = \"s3://comphrend-wallmart-bucket/input-data/wallmarts_tweets_1k.csv\"\n",
    "output_s3_url = \"s3://comphrend-wallmart-bucket/results\"\n",
    "input_doc_format = \"ONE_DOC_PER_LINE\"\n",
    "data_acess_role_arn = \"arn:aws:iam::893415859041:role/service-role/AmazonComprehendServiceRole-Com-S3\"\n",
    "\n",
    "input_data_config = {\"S3Uri\": input_s3_url, \"InputFormat\": input_doc_format}\n",
    "output_data_config = {\"S3Uri\": output_s3_url}\n",
    "\n",
    "start_job_sentiment = comprehend.start_sentiment_detection_job(\n",
    "    InputDataConfig=input_data_config,\n",
    "    OutputDataConfig=output_data_config,\n",
    "    DataAccessRoleArn=data_acess_role_arn,\n",
    "    LanguageCode='en',\n",
    "    JobName='Walmart_1K_tweets')\n",
    "\n",
    "job_id = start_job_sentiment['JobId']\n",
    "print(f\"Job ID: {job_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: SUBMITTED\n"
     ]
    }
   ],
   "source": [
    "describe_result = comprehend.describe_sentiment_detection_job(JobId=job_id)\n",
    "job_status = describe_result['SentimentDetectionJobProperties']['JobStatus']\n",
    "print(f'Job Status: {job_status}')\n",
    "if job_status == 'FAILED':\n",
    "    print(f'Reason: {describe_result[\"SentimentDetectionJobProperties\"][\"Message\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results S3 Url: s3://comphrend-wallmart-bucket/results/893415859041-SENTIMENT-7789fcaa43d6874376332374a69fe845/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "results_S3Url = comprehend.describe_sentiment_detection_job(\n",
    "    JobId=job_id)['SentimentDetectionJobProperties']['OutputDataConfig']['S3Uri']\n",
    "print(f'Results S3 Url: {results_S3Url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input-data/\n",
      "input-data/wallmarts_tweets_1k.csv\n",
      "results/\n",
      "results/.write_access_check_file.temp\n",
      "results/893415859041-SENTIMENT-7789fcaa43d6874376332374a69fe845/output/output.tar.gz\n",
      "results/893415859041-SENTIMENT-83481910f807237a7ca5738c53288d8d/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "def s3_bucket_list_obj(bucket):\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    for obj in bucket.objects.all():\n",
    "        print(obj.key)\n",
    "s3_bucket_list_obj(bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded to: ../Comprehend/outputs/sentiment.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Give your local results file a name\n",
    "results_name = 'sentiment'\n",
    "\n",
    "local_results_filename = '../Comprehend/outputs/' + results_name + '.tar.gz'\n",
    "s3_name = 's3://' + bucket_name + '/'\n",
    "results_aws_filename = results_S3Url.replace(s3_name, '')\n",
    "\n",
    "# Download results\n",
    "s3 = boto3.client('s3')\n",
    "s3.download_file(bucket_name,\n",
    "                results_aws_filename, \n",
    "                local_results_filename)\n",
    "\n",
    "print('Results downloaded to: ' + local_results_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results extracted to: Comprehend/outputs/extracted\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "def extract_targz(targz_file, output_path = ''):\n",
    "    if targz_file.endswith(\"tar.gz\"):\n",
    "        tar = tarfile.open(targz_file, \"r:gz\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "    elif targz_file.endswith(\"tar\"):\n",
    "        tar = tarfile.open(targz_file, \"r:\")\n",
    "        tar.extractall(path = output_path)\n",
    "        tar.close()\n",
    "results_name = 'sentiment'\n",
    "local_results_filename = '../Comprehend/outputs/' + results_name + '.tar.gz'\n",
    "output_path = 'Comprehend/outputs/extracted' \n",
    "extract_targz(local_results_filename, output_path)\n",
    "print('Results extracted to: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the output: 10000\n",
      "@lxoG21 I love me some Walmart candles lol the Sweet Apples and Cactus Aloe ðŸ¥´ their wax melts are amazing too\n",
      " {\n",
      "    \"File\": \"wallmarts_tweets_1k.csv\",\n",
      "    \"Line\": 5,\n",
      "    \"Sentiment\": \"NEUTRAL\",\n",
      "    \"SentimentScore\": {\n",
      "        \"Mixed\": 7.657633250346407e-05,\n",
      "        \"Negative\": 0.002575723920017481,\n",
      "        \"Neutral\": 0.9885324239730835,\n",
      "        \"Positive\": 0.008815310895442963\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Read JSON into a dictionary   \n",
    "import pandas as pd\n",
    "input_file = output_path + '/output'\n",
    "results = [json.loads(line) for line in open(input_file, 'r')]\n",
    "print('Number of records in the output:',len(results))\n",
    "\n",
    "user_input = input(\"Enter the tweet number to see the sentiment: \") \n",
    "df = pd.read_csv(local_file_name, header=None) \n",
    "output = json.dumps(results[int(user_input)], indent=4, sort_keys=True)\n",
    "print(df.iloc[int(user_input), 0] + '\\n', output)\n",
    "\n",
    "sentiment_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 21\u001b[0m\n\u001b[0;32m      3\u001b[0m record_no \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv(local_file_name, header=None)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# for i in range(records_no):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     print(df.loc[i] + '\\n', output_asynch)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     print('\\n')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTWEET TEXT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m---> 21\u001b[0m       \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mloc[record_no]\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Real Time Results\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mREAL TIME RESULTS:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "record_no = 1\n",
    "# df = pd.read_csv(local_file_name, header=None)\n",
    "\n",
    "# for i in range(records_no):\n",
    "#     df.loc[i] = df.loc[i].str.replace(r'\\s+', ' ').str.strip()\n",
    "#     output = json.dumps(results[i], indent=4, sort_keys=True)\n",
    "#     print('SYNCHRONOUS RESULTS:')\n",
    "#     print(df.iloc[i, 0] + '\\n', output)\n",
    "#     print('\\n')\n",
    "#     print('Sentiment: ' + results[i]['Sentiment'])\n",
    "# print('\\nASYNCHRONOUS RESULTS:')\n",
    "\n",
    "# for i in range(records_no):\n",
    "#     output_asynch = json.dumps(results[i], indent=4, sort_keys=True) \n",
    "#     print(df.loc[i] + '\\n', output_asynch)\n",
    "#     print('\\n')\n",
    "\n",
    "print('TWEET TEXT:\\n', \n",
    "      df.loc[record_no].item())\n",
    "# Real Time Results\n",
    "print('\\nREAL TIME RESULTS:\\n') \n",
    "print(comprehend.detect_sentiment(Text=df.loc[record_no].item(), LanguageCode='en')['SentimentScore'])\n",
    "# Job Resutls\n",
    "print('\\nASYNCHRONOUS RESULTS:')\n",
    "game = json.dumps(results[record_no], indent=4, sort_keys=True)\n",
    "print(game)\n",
    "\n",
    "#  write the results to a json file as current date and time \n",
    "output_file_name_json = '../Outputs_file/Wallmarts' + str({record_no}) + datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\") + '.json' \n",
    "\n",
    "with open(output_file_name_json, 'w') as f:\n",
    "      json.dump(results, f, indent=4, sort_keys=True) \n",
    "print('Results saved to: ' + output_file_name_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_name = '../Outputs_file/Wallmarts/' + str({record_no}) + datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\") + '.json'\n",
    "sentiment_results['Text'] = df.walmart_tweets\n",
    "# Convert json file to excel \n",
    "data_excel = pd.read_json(output_file_name_json).to_excel(output_name, engine = 'xlsxwriter',  encoding = 'utf-8')\n",
    "print('Results saved to: ' + output_name) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
