{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            ***  Asynchronous Processing - Scheduling an Analysis Job  ***\n",
    "I highly recommend that you run at least one Comprehend job from the point and click interface, especially, if you are new to AWS. This way you can create a data access role (aka data_access_role_arn), and then you can simply copy the role name from the job description.\n",
    "\n",
    "You will need to create your S3 bucket through the web interface or through BOTO3 API.\n",
    "\n",
    "Note that I use different folders s3://comprehend-api/input-data and s3://comprehend-api/results for input data and results output. This way, your results are not going to get confused for inputs if you were to analyze all files in the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "import json \n",
    "import pandas as pd \n",
    "from botocore.exceptions import ClientError \n",
    "load_dotenv()\n",
    "# print(load_dotenv())\n",
    "\n",
    "local_file_name = '../wallmarts_tweets.csv'\n",
    "bucket_name = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
